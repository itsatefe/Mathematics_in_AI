{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e0327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import svd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from PreProcess import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9abd785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionWithNoise:\n",
    "    def __init__(self, A, y, is_y_inconsistent=False, scale=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the Linear Regression model with the given dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        A (numpy.ndarray): Feature matrix.\n",
    "        y (numpy.ndarray): Target variable.\n",
    "        is_y_inconsistent (bool): Whether to introduce inconsistency in y.\n",
    "        scale (float): Scale of inconsistency if applied.\n",
    "        \"\"\"\n",
    "        self.A = np.hstack([np.ones((A.shape[0], 1)), A])  # Add intercept column\n",
    "        self.y = y.reshape(-1, 1)\n",
    "        self.is_y_inconsistent = is_y_inconsistent\n",
    "        \n",
    "        if self.is_y_inconsistent:\n",
    "            self.y = self.make_y_inconsistent(scale)\n",
    "            print(\"Using inconsistent y for training.\")\n",
    "        else:\n",
    "            print(\"Using consistent y for training.\")\n",
    "        \n",
    "        # Split data into training and testing sets (80% train, 20% test)\n",
    "        self.A_train, self.A_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.A, self.y, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        self.is_full_col_rank = np.linalg.matrix_rank(self.A_train) == self.A_train.shape[1]\n",
    "        \n",
    "        if self.is_full_col_rank:\n",
    "            print(\"Matrix A_train is full column rank. Proceeding with standard least squares solution.\")\n",
    "        else:\n",
    "            print(\"Matrix A_train is not full column rank. Using SVD-based pseudo-inverse.\")\n",
    "        \n",
    "    def make_y_inconsistent(self, scale):\n",
    "        \"\"\"\n",
    "        Modifies y to ensure inconsistency by adding a component orthogonal to A.\n",
    "        \"\"\"\n",
    "        U, S, Vt = svd(self.A, full_matrices=True)\n",
    "        null_space_vectors = U[:, len(S):]\n",
    "\n",
    "        if null_space_vectors.shape[1] == 0:\n",
    "            print(\"Warning: A has full rank, hard to create inconsistency.\")\n",
    "            return self.y\n",
    "        \n",
    "        random_null_component = null_space_vectors @ np.random.randn(null_space_vectors.shape[1], 1)\n",
    "        return self.y + scale * random_null_component\n",
    "    \n",
    "    def compute_pseudo_inverse(self, A):\n",
    "        \"\"\"\n",
    "        Computes the pseudo-inverse of A.\n",
    "        \"\"\"\n",
    "        rank = np.linalg.matrix_rank(A)\n",
    "        if rank == A.shape[1]:\n",
    "            try:\n",
    "                return np.linalg.inv(A.T @ A) @ A.T\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Unexpected singular matrix, using SVD instead.\")\n",
    "                return self.compute_generalized_inverse(A)\n",
    "        else:\n",
    "            return self.compute_generalized_inverse(A)\n",
    "    \n",
    "    def compute_generalized_inverse(self, A):\n",
    "        \"\"\"\n",
    "        Computes the Moore-Penrose pseudo-inverse using SVD.\n",
    "        \"\"\"\n",
    "        U, S, Vt = svd(A, full_matrices=False)\n",
    "        S_inv = np.diag([1/s if s > 1e-10 else 0 for s in S])\n",
    "        return Vt.T @ S_inv @ U.T\n",
    "    \n",
    "    def solve_linear_regression(self):\n",
    "        \"\"\"\n",
    "        Solves the linear regression problem using a manually computed pseudo-inverse.\n",
    "        \"\"\"\n",
    "        A_pinv_train = self.compute_pseudo_inverse(self.A_train)\n",
    "        return A_pinv_train @ self.y_train\n",
    "    \n",
    "    def correct_regression_coefficients(self, x0):\n",
    "        \"\"\"\n",
    "        Computes corrected regression coefficients to handle inconsistency.\n",
    "        \"\"\"\n",
    "        y_hat_train = self.A_train @ x0\n",
    "        e_out = y_hat_train - self.y_train\n",
    "        A_pinv_train = self.compute_pseudo_inverse(self.A_train)\n",
    "        e_in = A_pinv_train @ e_out\n",
    "        x_corrected = x0 + e_in\n",
    "        mse_corrected = np.mean((self.y_train - self.A_train @ x_corrected) ** 2)\n",
    "        return x_corrected, mse_corrected\n",
    "    \n",
    "    def run_regression(self):\n",
    "        x0 = self.solve_linear_regression()\n",
    "        print(\"Initial Linear Regression Coefficients (including intercept):\")\n",
    "        print(x0)\n",
    "        \n",
    "        if self.is_y_inconsistent:\n",
    "            x_corrected, mse_corrected = self.correct_regression_coefficients(x0)\n",
    "            print(\"Corrected Linear Regression Coefficients (including intercept):\")\n",
    "            print(x_corrected)\n",
    "            print(f\"Corrected Train MSE: {mse_corrected}\")\n",
    "            self.x_final = x_corrected\n",
    "        else:\n",
    "            y_hat_train = self.A_train @ x0\n",
    "            mse_train = np.mean((self.y_train - y_hat_train) ** 2)\n",
    "            print(f\"Train MSE (Standard Regression - y is consistent): {mse_train}\")\n",
    "            self.x_final = x0\n",
    "        \n",
    "        # Predict on test set and evaluate\n",
    "        y_hat_test = self.A_test @ self.x_final\n",
    "        mse_test = np.mean((self.y_test - y_hat_test) ** 2)\n",
    "        print(f\"Test MSE: {mse_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0669d0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using inconsistent y for training.\n",
      "Matrix A_train is full column rank. Proceeding with standard least squares solution.\n",
      "Initial Linear Regression Coefficients (including intercept):\n",
      "[[22.02180094]\n",
      " [ 2.94234333]\n",
      " [-0.49567393]\n",
      " [-0.9987891 ]\n",
      " [-5.29356655]]\n",
      "Corrected Linear Regression Coefficients (including intercept):\n",
      "[[22.02180094]\n",
      " [ 2.94234333]\n",
      " [-0.49567393]\n",
      " [-0.9987891 ]\n",
      " [-5.29356655]]\n",
      "Corrected Train MSE: 22.926613200707987\n",
      "Test MSE: 21.92022512203619\n"
     ]
    }
   ],
   "source": [
    "df = preprocess(\"housing_data.csv\", \"MEDV\", [\"RM\", \"AGE\", \"DIS\", \"LSTAT\"])\n",
    "y = df[\"MEDV\"].values\n",
    "df = df.drop(columns=[\"MEDV\"])\n",
    "A = df.to_numpy()\n",
    "\n",
    "model = LinearRegressionWithNoise(A, y, is_y_inconsistent=True)\n",
    "model.run_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf31a8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
